{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import inflection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from boruta import BorutaPy\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy import stats as ss\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 - Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jupyter_settings():\n",
    "    plt.style.use(\"bmh\")\n",
    "    plt.rcParams[\"figure.figsize\"] = [25, 12]\n",
    "    plt.rcParams[\"font.size\"] = 24\n",
    "\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    pd.options.display.max_columns = 50\n",
    "    pd.options.display.max_rows = 50\n",
    "    pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "    sns.set()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cramer V function\n",
    "def cramer_v(x, y):\n",
    "    cm = pd.crosstab(x, y).values\n",
    "    n = cm.sum()\n",
    "    r, k = cm.shape\n",
    "    chi2 = ss.chi2_contingency(cm)[0]\n",
    "    chi2corr = max(0, chi2 - (k - 1) * (r - 1) / (n - 1))\n",
    "    kcorr = k - (k - 1) ** 2 / (n - 1)\n",
    "    rcorr = r - (r - 1) ** 2 / (n - 1)\n",
    "\n",
    "    return np.sqrt((chi2corr / n) / (min(kcorr - 1, rcorr - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset functions\n",
    "def rename_cols(dataframe):\n",
    "    # Rename columns\n",
    "    cols_old = list(dataframe)\n",
    "    # Function from rename\n",
    "    snakecase = lambda x: inflection.underscore(x)\n",
    "    cols_new = list(map(snakecase, cols_old))\n",
    "    # Renaming columns\n",
    "    dataframe.columns = cols_new\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def fillout_na(dataframe):\n",
    "    # Fix date type\n",
    "    dataframe[\"date\"] = pd.to_datetime(dataframe[\"date\"])\n",
    "\n",
    "    # Fillout Na's\n",
    "    dataframe[\"competition_distance\"] = dataframe[\"competition_distance\"].apply(\n",
    "        lambda x: 200000.0 if math.isnan(x) else x\n",
    "    )\n",
    "    # competition_open_since_month\n",
    "    dataframe[\"competition_open_since_month\"] = dataframe.apply(\n",
    "        lambda x: x[\"date\"].month\n",
    "        if math.isnan(x[\"competition_open_since_month\"])\n",
    "        else x[\"competition_open_since_month\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    # competition_open_since_year\n",
    "    dataframe[\"competition_open_since_year\"] = dataframe.apply(\n",
    "        lambda x: x[\"date\"].year\n",
    "        if math.isnan(x[\"competition_open_since_year\"])\n",
    "        else x[\"competition_open_since_year\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    # promo2_since_week\n",
    "    dataframe[\"promo2_since_week\"] = dataframe.apply(\n",
    "        lambda x: x[\"date\"].week\n",
    "        if math.isnan(x[\"promo2_since_week\"])\n",
    "        else x[\"promo2_since_week\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    # promo2_since_year\n",
    "    dataframe[\"promo2_since_year\"] = dataframe.apply(\n",
    "        lambda x: x[\"date\"].year\n",
    "        if math.isnan(x[\"promo2_since_year\"])\n",
    "        else x[\"promo2_since_year\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    # promo_interval\n",
    "    month_map = {\n",
    "        1: \"Jan\",\n",
    "        2: \"Fev\",\n",
    "        3: \"Mar\",\n",
    "        4: \"Apr\",\n",
    "        5: \"May\",\n",
    "        6: \"Jun\",\n",
    "        7: \"Jul\",\n",
    "        8: \"Aug\",\n",
    "        9: \"Sep\",\n",
    "        10: \"Oct\",\n",
    "        11: \"Nov\",\n",
    "        12: \"Dec\",\n",
    "    }\n",
    "    dataframe[\"promo_interval\"].fillna(0, inplace=True)\n",
    "    dataframe[\"month_map\"] = dataframe[\"date\"].dt.month.map(month_map)\n",
    "    dataframe[\"is_promo\"] = dataframe[[\"promo_interval\", \"month_map\"]].apply(\n",
    "        lambda x: 0\n",
    "        if x[\"promo_interval\"] == 0\n",
    "        else 1\n",
    "        if x[\"month_map\"] in x[\"promo_interval\"].split(\",\")\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def change_types(dataframe):\n",
    "    dataframe[\"competition_open_since_month\"] = dataframe[\n",
    "        \"competition_open_since_month\"\n",
    "    ].astype(int)\n",
    "    dataframe[\"competition_open_since_year\"] = dataframe[\n",
    "        \"competition_open_since_year\"\n",
    "    ].astype(int)\n",
    "    dataframe[\"promo2_since_week\"] = dataframe[\"promo2_since_week\"].astype(int)\n",
    "    dataframe[\"promo2_since_year\"] = dataframe[\"promo2_since_year\"].astype(int)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def feature_eng(dataframe):\n",
    "    # Creating datetimes\n",
    "    # Year\n",
    "    dataframe[\"year\"] = dataframe[\"date\"].dt.year\n",
    "\n",
    "    # Month\n",
    "    dataframe[\"month\"] = dataframe[\"date\"].dt.month\n",
    "\n",
    "    # Day\n",
    "    dataframe[\"day\"] = dataframe[\"date\"].dt.day\n",
    "\n",
    "    # Week of Year\n",
    "    dataframe[\"week_of_year\"] = dataframe[\"date\"].dt.isocalendar().week\n",
    "\n",
    "    # Year Week\n",
    "    dataframe[\"year_week\"] = dataframe[\"date\"].dt.strftime(\"%Y-%W\")\n",
    "\n",
    "    dataframe[\"competition_since\"] = dataframe.apply(\n",
    "        lambda x: datetime.datetime(\n",
    "            year=x[\"competition_open_since_year\"],\n",
    "            month=x[\"competition_open_since_month\"],\n",
    "            day=1,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    dataframe[\"competition_time_month\"] = (\n",
    "        ((dataframe[\"date\"] - dataframe[\"competition_since\"]) / 30)\n",
    "        .apply(lambda x: x.days)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Promo Since\n",
    "    dataframe[\"promo_since\"] = (\n",
    "        dataframe[\"promo2_since_year\"].astype(str)\n",
    "        + \"-\"\n",
    "        + dataframe[\"promo2_since_week\"].astype(str)\n",
    "    )\n",
    "    dataframe[\"promo_since\"] = dataframe[\"promo_since\"].apply(\n",
    "        lambda x: datetime.datetime.strptime(x + \"-1\", \"%Y-%W-%w\")\n",
    "        - datetime.timedelta(days=7)\n",
    "    )\n",
    "    dataframe[\"promo_time_week\"] = (\n",
    "        ((dataframe[\"date\"] - dataframe[\"promo_since\"]) / 7)\n",
    "        .apply(lambda x: x.days)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Assortment\n",
    "    dataframe[\"assortment\"] = dataframe[\"assortment\"].apply(\n",
    "        lambda x: \"basic\" if x == \"a\" else \"extra\" if x == \"b\" else \"extended\"\n",
    "    )\n",
    "\n",
    "    # State Holiday\n",
    "    dataframe[\"state_holiday\"] = dataframe[\"state_holiday\"].apply(\n",
    "        lambda x: \"public_holiday\"\n",
    "        if x == \"a\"\n",
    "        else \"easter_holiday\"\n",
    "        if x == \"b\"\n",
    "        else \"christmas\"\n",
    "        if x == \"c\"\n",
    "        else \"regular_day\"\n",
    "    )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def filter_features(dataframe):\n",
    "    # Filter lines\n",
    "    dataframe = dataframe.loc[(dataframe[\"open\"] != 0) & (dataframe[\"sales\"] > 0), :]\n",
    "\n",
    "    # Filter columns\n",
    "    dataframe = dataframe.drop(\n",
    "        [\"customers\", \"open\", \"promo_interval\", \"month_map\"], axis=1\n",
    "    )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def data_preparation(dataframe):\n",
    "    # Rescaling\n",
    "    rs = RobustScaler()\n",
    "    mms = MinMaxScaler()\n",
    "\n",
    "    # competition distance\n",
    "    dataframe[\"competition_distance\"] = rs.fit_transform(\n",
    "        dataframe[[\"competition_distance\"]].values\n",
    "    )\n",
    "\n",
    "    # competition time month\n",
    "    dataframe[\"competition_time_month\"] = rs.fit_transform(\n",
    "        dataframe[[\"competition_time_month\"]].values\n",
    "    )\n",
    "\n",
    "    # promo time week\n",
    "    dataframe[\"promo_time_week\"] = mms.fit_transform(\n",
    "        dataframe[[\"promo_time_week\"]].values\n",
    "    )\n",
    "\n",
    "    # year\n",
    "    dataframe[\"year\"] = mms.fit_transform(dataframe[[\"year\"]].values)\n",
    "\n",
    "    # Encoding\n",
    "    # state holiday - One Hot Encoding\n",
    "    dataframe = pd.get_dummies(\n",
    "        dataframe, prefix=[\"state_holiday\"], columns=[\"state_holiday\"]\n",
    "    )\n",
    "\n",
    "    # store type - Label Encoder\n",
    "    le = LabelEncoder()\n",
    "    dataframe[\"store_type\"] = le.fit_transform(dataframe[\"store_type\"])\n",
    "\n",
    "    # assortment\n",
    "    assortment_dict = {\n",
    "        \"basic\": 1,\n",
    "        \"extra\": 2,\n",
    "        \"extended\": 3,\n",
    "    }\n",
    "\n",
    "    dataframe[\"assortment\"] = dataframe[\"assortment\"].map(assortment_dict)\n",
    "\n",
    "    # Target Variable Transformation\n",
    "    dataframe[\"sales\"] = np.log1p(dataframe[\"sales\"])\n",
    "\n",
    "    # Natural Transformation\n",
    "    # month\n",
    "    dataframe[\"month_sin\"] = dataframe[\"month\"].apply(\n",
    "        lambda x: np.sin(x * (2.0 * np.pi / 12))\n",
    "    )\n",
    "    dataframe[\"month_cos\"] = dataframe[\"month\"].apply(\n",
    "        lambda x: np.cos(x * (2.0 * np.pi / 12))\n",
    "    )\n",
    "\n",
    "    # day\n",
    "    dataframe[\"day_sin\"] = dataframe[\"day\"].apply(\n",
    "        lambda x: np.sin(x * (2.0 * np.pi / 30))\n",
    "    )\n",
    "    dataframe[\"day_cos\"] = dataframe[\"day\"].apply(\n",
    "        lambda x: np.cos(x * (2.0 * np.pi / 30))\n",
    "    )\n",
    "\n",
    "    # day of week\n",
    "    dataframe[\"day_of_week_sin\"] = dataframe[\"day_of_week\"].apply(\n",
    "        lambda x: np.sin(x * (2.0 * np.pi / 7))\n",
    "    )\n",
    "    dataframe[\"day_of_week_cos\"] = dataframe[\"day_of_week\"].apply(\n",
    "        lambda x: np.cos(x * (2.0 * np.pi / 7))\n",
    "    )\n",
    "\n",
    "    # week of year\n",
    "    dataframe[\"week_of_year_sin\"] = dataframe[\"week_of_year\"].apply(\n",
    "        lambda x: np.sin(x * (2.0 * np.pi / 52))\n",
    "    )\n",
    "    dataframe[\"week_of_year_cos\"] = dataframe[\"week_of_year\"].apply(\n",
    "        lambda x: np.cos(x * (2.0 * np.pi / 52))\n",
    "    )\n",
    "\n",
    "    dataframe = dataframe.drop(\n",
    "        [\n",
    "            \"day_of_week\",\n",
    "            \"day\",\n",
    "            \"month\",\n",
    "            \"week_of_year\",\n",
    "            \"promo_since\",\n",
    "            \"competition_since\",\n",
    "            \"year_week\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "def ml_error(model_name, y, yhat):\n",
    "    mae = mean_absolute_error(y, yhat)\n",
    "    mape = mean_absolute_percentage_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Model Name\": model_name,\n",
    "            \"MAE\": np.round(mae, 2),\n",
    "            \"MAPE\": np.round(mape, 4),\n",
    "            \"RMSE\": np.round(rmse, 2),\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "\n",
    "def mean_percentage_error(y, y_hat):\n",
    "    return np.mean((y - y_hat) / y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series cross validation function\n",
    "def cross_validation(X_train, kfold, model_name, model, verbose=False):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    for k in reversed(range(1, kfold + 1)):\n",
    "        # Start and end validation date\n",
    "        validation_start_date = X_train[\"date\"].max() - datetime.timedelta(\n",
    "            days=k * 6 * 7\n",
    "        )\n",
    "        validation_end_date = X_train[\"date\"].max() - datetime.timedelta(\n",
    "            days=(k - 1) * 6 * 7\n",
    "        )\n",
    "\n",
    "        # filtering dataset\n",
    "        training = X_train[X_train[\"date\"] < validation_start_date]\n",
    "        validation = X_train[\n",
    "            (X_train[\"date\"] >= validation_start_date)\n",
    "            & (X_train[\"date\"] <= validation_end_date)\n",
    "        ]\n",
    "\n",
    "        # training and validation dataset\n",
    "        xtraining = training.drop([\"date\", \"sales\"], axis=1)\n",
    "        ytraining = training[\"sales\"]\n",
    "\n",
    "        xvalidation = validation.drop([\"date\", \"sales\"], axis=1)\n",
    "        yvalidation = validation[\"sales\"]\n",
    "\n",
    "        # model\n",
    "        m = model.fit(xtraining, ytraining)\n",
    "\n",
    "        # predict\n",
    "        yhat_m = m.predict(xvalidation)\n",
    "\n",
    "        # performance\n",
    "        m_performance = ml_error(model_name, np.expm1(yvalidation), np.expm1(yhat_m))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"K-Fold: {k}\")\n",
    "            print(f\"\\n{m_performance}\")\n",
    "\n",
    "        # store performance for each kfold\n",
    "        mae_list.append(m_performance[\"MAE\"])\n",
    "        mape_list.append(m_performance[\"MAPE\"])\n",
    "        rmse_list.append(m_performance[\"RMSE\"])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Model Name\": model_name,\n",
    "            \"MAE CV\": np.round(np.mean(mae_list), 2).astype(str)\n",
    "            + \" +/- \"\n",
    "            + np.round(np.std(mae_list), 2).astype(str),\n",
    "            \"MAPE CV\": np.round(np.mean(mape_list), 4).astype(str)\n",
    "            + \" +/- \"\n",
    "            + np.round(np.std(mape_list), 4).astype(str),\n",
    "            \"RMSE CV\": np.round(np.mean(rmse_list), 2).astype(str)\n",
    "            + \" +/- \"\n",
    "            + np.round(np.std(rmse_list), 2).astype(str),\n",
    "        },\n",
    "        index=[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df_store_raw = pd.read_csv(\"../data/raw/store.csv\")\n",
    "\n",
    "# Merging datasets\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how=\"left\", on=\"Store\")\n",
    "\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 - Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating 6 weeks for test dataset\n",
    "df_raw[\"Date\"] = pd.to_datetime(df_raw[\"Date\"])\n",
    "\n",
    "df_raw[[\"Store\", \"Date\"]].groupby(\"Store\").max().reset_index()[\"Date\"][\n",
    "    0\n",
    "] - datetime.timedelta(days=6 * 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "train = df_raw.loc[df_raw[\"Date\"] < \"2015-06-19\", :]\n",
    "test = df_raw.loc[df_raw[\"Date\"] >= \"2015-06-19\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Min Date: {}\".format(train[\"Date\"].min()))\n",
    "print(\"Training Max Date: {}\".format(train[\"Date\"].max()))\n",
    "\n",
    "print(\"\\nTest Min Date: {}\".format(test[\"Date\"].min()))\n",
    "print(\"Test Max Date: {}\".format(test[\"Date\"].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 - Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset copy\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting columns names on list\n",
    "cols_old = list(df1)\n",
    "\n",
    "# Function from rename\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "cols_new = list(map(snakecase, cols_old))\n",
    "\n",
    "# Renaming columns\n",
    "df1.columns = cols_new\n",
    "\n",
    "# New columns names\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Data Dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Rows and Columns\n",
    "print(f\"Number of Rows: {df1.shape[0]}\")\n",
    "print(f\"Number of Columns: {df1.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Check Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking types\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Check NA's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking sum of Na's\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Fillout NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_distance\n",
    "\n",
    "# Fill with a big number that represents no competition\n",
    "df1[\"competition_distance\"] = df1[\"competition_distance\"].apply(\n",
    "    lambda x: 200000.0 if math.isnan(x) else x\n",
    ")\n",
    "# competition_open_since_month\n",
    "\n",
    "# Fill with record date if there is no competition so de fiference from date to competition open is zero\n",
    "df1[\"competition_open_since_month\"] = df1.apply(\n",
    "    lambda x: x[\"date\"].month\n",
    "    if math.isnan(x[\"competition_open_since_month\"])\n",
    "    else x[\"competition_open_since_month\"],\n",
    "    axis=1,\n",
    ")\n",
    "# competition_open_since_year\n",
    "df1[\"competition_open_since_year\"] = df1.apply(\n",
    "    lambda x: x[\"date\"].year\n",
    "    if math.isnan(x[\"competition_open_since_year\"])\n",
    "    else x[\"competition_open_since_year\"],\n",
    "    axis=1,\n",
    ")\n",
    "# promo2_since_week\n",
    "df1[\"promo2_since_week\"] = df1.apply(\n",
    "    lambda x: x[\"date\"].week\n",
    "    if math.isnan(x[\"promo2_since_week\"])\n",
    "    else x[\"promo2_since_week\"],\n",
    "    axis=1,\n",
    ")\n",
    "# promo2_since_year\n",
    "df1[\"promo2_since_year\"] = df1.apply(\n",
    "    lambda x: x[\"date\"].year\n",
    "    if math.isnan(x[\"promo2_since_year\"])\n",
    "    else x[\"promo2_since_year\"],\n",
    "    axis=1,\n",
    ")\n",
    "# promo_interval\n",
    "month_map = {\n",
    "    1: \"Jan\",\n",
    "    2: \"Fev\",\n",
    "    3: \"Mar\",\n",
    "    4: \"Apr\",\n",
    "    5: \"May\",\n",
    "    6: \"Jun\",\n",
    "    7: \"Jul\",\n",
    "    8: \"Aug\",\n",
    "    9: \"Sep\",\n",
    "    10: \"Oct\",\n",
    "    11: \"Nov\",\n",
    "    12: \"Dec\",\n",
    "}\n",
    "df1[\"promo_interval\"].fillna(0, inplace=True)\n",
    "df1[\"month_map\"] = df1[\"date\"].dt.month.map(month_map)\n",
    "df1[\"is_promo\"] = df1[[\"promo_interval\", \"month_map\"]].apply(\n",
    "    lambda x: 0\n",
    "    if x[\"promo_interval\"] == 0\n",
    "    else 1\n",
    "    if x[\"month_map\"] in x[\"promo_interval\"].split(\",\")\n",
    "    else 0,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 - Change Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"competition_open_since_month\"] = df1[\"competition_open_since_month\"].astype(int)\n",
    "df1[\"competition_open_since_year\"] = df1[\"competition_open_since_year\"].astype(int)\n",
    "\n",
    "df1[\"promo2_since_week\"] = df1[\"promo2_since_week\"].astype(int)\n",
    "df1[\"promo2_since_year\"] = df1[\"promo2_since_year\"].astype(int)\n",
    "\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 - Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating numeric from categorical variables\n",
    "num_attributes = df1.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "cat_attributes = df1.select_dtypes(exclude=[\"int64\", \"float64\", \"datetime64[ns]\"])\n",
    "\n",
    "cat_attributes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 - Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central tendencies, mean and median\n",
    "ct_mean = pd.DataFrame(num_attributes.apply(np.mean)).T\n",
    "ct_median = pd.DataFrame(num_attributes.apply(np.median)).T\n",
    "\n",
    "# Dispersion, std, min, max, range, skew and kurtosis\n",
    "d_std = pd.DataFrame(num_attributes.apply(np.std)).T\n",
    "d_min = pd.DataFrame(num_attributes.apply(min)).T\n",
    "d_max = pd.DataFrame(num_attributes.apply(max)).T\n",
    "d_range = pd.DataFrame(num_attributes.apply(lambda x: x.max() - x.min())).T\n",
    "d_skew = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).T\n",
    "d_kurtosis = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).T\n",
    "\n",
    "# Concatenate\n",
    "m = pd.concat(\n",
    "    [d_min, d_max, d_range, ct_mean, ct_median, d_std, d_skew, d_kurtosis]\n",
    ").T.reset_index()\n",
    "m.columns = [\n",
    "    \"features\",\n",
    "    \"min\",\n",
    "    \"max\",\n",
    "    \"range\",\n",
    "    \"mean\",\n",
    "    \"median\",\n",
    "    \"std\",\n",
    "    \"skew\",\n",
    "    \"kurtosis\",\n",
    "]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df1[\"sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df1[\"competition_distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 - Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes.apply(lambda x: x.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this data leakage?\n",
    "filters = (df1[\"state_holiday\"] != \"0\") & (df1[\"sales\"] > 0)\n",
    "aux = df1.loc[filters, :]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x=\"state_holiday\", y=\"sales\", data=aux)\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x=\"store_type\", y=\"sales\", data=aux)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(x=\"assortment\", y=\"sales\", data=aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Hypothesis Mind Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "mindmap\n",
    "    [Daily Stores Sales]\n",
    "        )Clients(\n",
    "            Volume of Sales\n",
    "            Number of Children\n",
    "            Income\n",
    "            Age\n",
    "            Profession\n",
    "            Family\n",
    "            Purchase Frequency\n",
    "        )Location(\n",
    "            School Area\n",
    "            Neighborhood\n",
    "            Rural\n",
    "            Urban\n",
    "            Downtown\n",
    "            Hospital Area\n",
    "        )Time(\n",
    "            Holiday\n",
    "            Week of Year\n",
    "            Day\n",
    "            Month\n",
    "            Year\n",
    "            Hour\n",
    "            Weekend\n",
    "            Sale\n",
    "        )Stores(\n",
    "            Staff Size\n",
    "            Supply\n",
    "            Size\n",
    "            Assortment\n",
    "            Competition\n",
    "        )Products(\n",
    "            Marketing\n",
    "            Disposition\n",
    "            Price\n",
    "            Supply\n",
    "            Sale\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - **Store** Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Stores with more employess should sell more.  \n",
    "**2.** Stores with bigger storage should sell more.  \n",
    "**3.** Bigger stores should sell more.  \n",
    "**4.** Stores with higher assortment should sell more.  \n",
    "**5.** Stores with closer competitors should sell less.  \n",
    "**6.** Stores with older competitors should sell more.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - **Products** Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Sores with higher marketing spending should sell more.  \n",
    "**2.** Stores with bigger product display should sell more.  \n",
    "**3.** Stores with cheaper products should sell more.  \n",
    "**5.** Stores with more agressive discounts should sell more.  \n",
    "**6.** Stores with longer active promotions should sell more.  \n",
    "**7.** Stores with more promotion days should sell more.  \n",
    "**8.** Stores with more consecutive promotions should sell more.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 - **Time** Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Stores should sell more during christmas.  \n",
    "**2.** Stores should sell more throughout the years.  \n",
    "**3.** Stores should sell more on the second semester.  \n",
    "**4.** Stores should sell more after 10th day of each month.  \n",
    "**5.** Stores should sell less on weekends.  \n",
    "**6.** Stores should sell less during school holidays.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Final Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Stores with higher assortment should sell more.  \n",
    "**2.** Stores with closer competitors should sell less.  \n",
    "**3.** Stores with older competitors should sell more.  \n",
    "**4.** Stores with longer active promotions should sell more.  \n",
    "**5.** Stores with more promotion days should sell more.  \n",
    "**6.** Stores with more consecutive promotions should sell more.  \n",
    "**7.** Stores open during christmas should sell more.  \n",
    "**8.** Stores should sell more throughout the years.  \n",
    "**9.** Stores should sell more on the second semester.  \n",
    "**10.** Stores should sell more after 10th day of each month.  \n",
    "**11.** Stores should sell less on weekends.  \n",
    "**12.** Stores should sell less during school holidays.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datetimes\n",
    "\n",
    "# Year\n",
    "df2[\"year\"] = df2[\"date\"].dt.year\n",
    "\n",
    "# Month\n",
    "df2[\"month\"] = df2[\"date\"].dt.month\n",
    "\n",
    "# Day\n",
    "df2[\"day\"] = df2[\"date\"].dt.day\n",
    "\n",
    "# Week of Year\n",
    "df2[\"week_of_year\"] = df2[\"date\"].dt.isocalendar().week\n",
    "\n",
    "# Year Week\n",
    "df2[\"year_week\"] = df2[\"date\"].dt.strftime(\"%Y-%W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition Since\n",
    "df2[\"competition_since\"] = df2.apply(\n",
    "    lambda x: datetime.datetime(\n",
    "        year=x[\"competition_open_since_year\"],\n",
    "        month=x[\"competition_open_since_month\"],\n",
    "        day=1,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "df2[\"competition_time_month\"] = (\n",
    "    ((df2[\"date\"] - df2[\"competition_since\"]) / 30).apply(lambda x: x.days).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promo Since\n",
    "df2[\"promo_since\"] = (\n",
    "    df2[\"promo2_since_year\"].astype(str) + \"-\" + df2[\"promo2_since_week\"].astype(str)\n",
    ")\n",
    "df2[\"promo_since\"] = df2[\"promo_since\"].apply(\n",
    "    lambda x: datetime.datetime.strptime(x + \"-1\", \"%Y-%W-%w\")\n",
    "    - datetime.timedelta(days=7)\n",
    ")\n",
    "df2[\"promo_time_week\"] = (\n",
    "    ((df2[\"date\"] - df2[\"promo_since\"]) / 7).apply(lambda x: x.days).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortment\n",
    "df2[\"assortment\"] = df2[\"assortment\"].apply(\n",
    "    lambda x: \"basic\" if x == \"a\" else \"extra\" if x == \"b\" else \"extended\"\n",
    ")\n",
    "\n",
    "# State Holiday\n",
    "df2[\"state_holiday\"] = df2[\"state_holiday\"].apply(\n",
    "    lambda x: \"public_holiday\"\n",
    "    if x == \"a\"\n",
    "    else \"easter_holiday\"\n",
    "    if x == \"b\"\n",
    "    else \"christmas\"\n",
    "    if x == \"c\"\n",
    "    else \"regular_day\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 - Feature Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.loc[(df3[\"open\"] != 0) & (df3[\"sales\"] > 0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Filtering Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected only open stores and number of customers is a \"leaky\" variable\n",
    "df3 = df3.drop([\"customers\", \"open\", \"promo_interval\", \"month_map\"], axis=1)\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 - Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df4[\"sales\"], kde=True, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 - Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes.hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 - Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"state_holiday\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"store_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"assortment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "plt.subplot(3, 2, 1)\n",
    "df_aux = df4.loc[df4[\"state_holiday\"] != \"regular_day\"]\n",
    "sns.countplot(x=df_aux[\"state_holiday\"])\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.kdeplot(\n",
    "    df4[df4[\"state_holiday\"] == \"public_holiday\"][\"sales\"],\n",
    "    label=\"public_holiday\",\n",
    "    fill=True,\n",
    ").legend(loc=\"upper right\")\n",
    "sns.kdeplot(\n",
    "    df4[df4[\"state_holiday\"] == \"easter_holiday\"][\"sales\"],\n",
    "    label=\"easter_holiday\",\n",
    "    fill=True,\n",
    ").legend(loc=\"upper right\")\n",
    "sns.kdeplot(\n",
    "    df4[df4[\"state_holiday\"] == \"christmas\"][\"sales\"], label=\"christmas\", fill=True\n",
    ").legend(loc=\"upper right\")\n",
    "\n",
    "# store_type\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.countplot(x=df4[\"store_type\"])\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.kdeplot(df4[df4[\"store_type\"] == \"a\"][\"sales\"], label=\"a\", fill=True).legend(\n",
    "    loc=\"upper right\"\n",
    ")\n",
    "sns.kdeplot(df4[df4[\"store_type\"] == \"b\"][\"sales\"], label=\"b\", fill=True).legend(\n",
    "    loc=\"upper right\"\n",
    ")\n",
    "sns.kdeplot(df4[df4[\"store_type\"] == \"c\"][\"sales\"], label=\"c\", fill=True).legend(\n",
    "    loc=\"upper right\"\n",
    ")\n",
    "sns.kdeplot(df4[df4[\"store_type\"] == \"d\"][\"sales\"], label=\"d\", fill=True).legend(\n",
    "    loc=\"upper right\"\n",
    ")\n",
    "\n",
    "# assortment\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.countplot(x=df4[\"assortment\"])\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.kdeplot(\n",
    "    df4[df4[\"assortment\"] == \"extended\"][\"sales\"], label=\"extended\", fill=True\n",
    ").legend(loc=\"upper right\")\n",
    "sns.kdeplot(\n",
    "    df4[df4[\"assortment\"] == \"basic\"][\"sales\"], label=\"basic\", fill=True\n",
    ").legend(loc=\"upper right\")\n",
    "sns.kdeplot(\n",
    "    df4[df4[\"assortment\"] == \"extra\"][\"sales\"], label=\"extra\", fill=True\n",
    ").legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H1.** Stores with higher assortment should sell more.  \n",
    "**TRUE:** Stores with **higher assortment**, on average, sell **more**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4.loc[:, [\"assortment\", \"sales\"]].groupby([\"assortment\"]).mean().reset_index()\n",
    "sns.barplot(x=\"assortment\", y=\"sales\", data=aux1, hue=\"assortment\")\n",
    "\n",
    "aux2 = (\n",
    "    df4.loc[:, [\"year_week\", \"assortment\", \"sales\"]]\n",
    "    .groupby([\"year_week\", \"assortment\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "aux2.pivot(index=\"year_week\", columns=\"assortment\", values=\"sales\").plot()\n",
    "\n",
    "aux3 = aux2.loc[aux2[\"assortment\"] == \"extra\", :]\n",
    "aux3.pivot(index=\"year_week\", columns=\"assortment\", values=\"sales\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H2.** Store with close competitores should sell less.  \n",
    "**FALSE:** Theres **no clear correlation** between competition distance and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"competition_distance\", \"sales\"]]\n",
    "    .groupby([\"competition_distance\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "bins = list(np.arange(0, 30000, 1000))\n",
    "aux2 = aux1.copy()\n",
    "aux2[\"competition_distance_binned\"] = pd.cut(aux1[\"competition_distance\"], bins=bins)\n",
    "aux3 = (\n",
    "    aux2.loc[:, [\"competition_distance_binned\", \"sales\"]]\n",
    "    .groupby([\"competition_distance_binned\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x=\"competition_distance_binned\", y=\"sales\", data=aux3)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.heatmap(aux1.corr(method=\"pearson\"), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H3.** Stores with older competitors should sell more.  \n",
    "**FALSE:** Theres **no clear correlation** between competitors age and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"competition_time_month\", \"sales\"]]\n",
    "    .groupby([\"competition_time_month\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "aux2 = aux1.loc[\n",
    "    (aux1[\"competition_time_month\"] < 120) & (aux1[\"competition_time_month\"] != 0), :\n",
    "]\n",
    "\n",
    "grid = GridSpec(2, 2)\n",
    "\n",
    "plt.subplot(grid[0, :])\n",
    "sns.barplot(x=\"competition_time_month\", y=\"sales\", data=aux2)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(grid[1, 0])\n",
    "sns.regplot(x=\"competition_time_month\", y=\"sales\", data=aux2)\n",
    "\n",
    "plt.subplot(grid[1, 1])\n",
    "sns.heatmap(aux2.corr(method=\"pearson\"), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H4.** Stores with longer active promotions should sell more.  \n",
    "**FALSE:** Theres **no clear correlation** between promotions active time and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"promo_time_week\", \"sales\"]]\n",
    "    .groupby([\"promo_time_week\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "grid = GridSpec(3, 3)\n",
    "\n",
    "plt.subplot(grid[0, :])\n",
    "aux2 = aux1.loc[aux1[\"promo_time_week\"] > 0]  # promo extendido\n",
    "sns.barplot(x=\"promo_time_week\", y=\"sales\", data=aux2)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(grid[1, :])\n",
    "aux3 = aux1.loc[aux1[\"promo_time_week\"] < 0]  # promo regular\n",
    "sns.barplot(x=\"promo_time_week\", y=\"sales\", data=aux3)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(grid[2, 0])\n",
    "sns.regplot(x=\"promo_time_week\", y=\"sales\", data=aux2)\n",
    "\n",
    "plt.subplot(grid[2, 1])\n",
    "sns.regplot(x=\"promo_time_week\", y=\"sales\", data=aux3)\n",
    "\n",
    "plt.subplot(grid[2, 2])\n",
    "sns.heatmap(aux1.corr(method=\"pearson\"), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H5.** Stores with more promotion days should sell more.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H6.** Stores with more consecutive promotions should sell more.  \n",
    "**FALSE:** Stores with **consecutive promotions**, on average, sell **less**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[:, [\"promo\", \"promo2\", \"sales\"]].groupby(\n",
    "    [\"promo\", \"promo2\"]\n",
    ").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[(df4[\"promo\"] == 1) & (df4[\"promo2\"] == 1), [\"year_week\", \"sales\"]]\n",
    "    .groupby([\"year_week\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = (\n",
    "    df4.loc[(df4[\"promo\"] == 1) & (df4[\"promo2\"] == 0), [\"year_week\", \"sales\"]]\n",
    "    .groupby([\"year_week\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "aux2.plot(ax=ax)\n",
    "ax.legend(labels=[\"Tradicional & Extendida\", \"Extendida\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H7.** Stores should sell more on christmas.  \n",
    "**TRUE:** Stores sell **more during christmas** than regular days, on average.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"state_holiday\", \"sales\"]]\n",
    "    .groupby([\"state_holiday\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=\"state_holiday\", y=\"sales\", data=aux1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "aux2 = (\n",
    "    df4.loc[:, [\"year\", \"state_holiday\", \"sales\"]]\n",
    "    .groupby([\"year\", \"state_holiday\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sns.barplot(x=\"year\", y=\"sales\", hue=\"state_holiday\", data=aux2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H8.** Stores should sell more throughout the years.  \n",
    "**FALSE:** There is **no clear growth trend** in sales through the years. But there is a seasonality trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"year\", \"month\", \"sales\"]]\n",
    "    .groupby([\"year\", \"month\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "aux1[\"year\"] = aux1[\"year\"].astype(str)\n",
    "\n",
    "aux2 = (\n",
    "    df4.loc[:, [\"year\", \"month\", \"sales\"]]\n",
    "    .groupby([\"year\", \"month\"])\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")\n",
    "aux2[\"year\"] = aux2[\"year\"].astype(str)\n",
    "\n",
    "aux3 = (\n",
    "    df4.loc[:, [\"store\", \"year\", \"month\", \"sales\"]]\n",
    "    .groupby([\"store\", \"year\", \"month\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "aux3[\"date\"] = pd.to_datetime(aux3[[\"year\", \"month\"]].assign(DAY=1))\n",
    "aux3 = aux3.drop([\"year\", \"month\"], axis=1)\n",
    "\n",
    "grid = GridSpec(2, 2)\n",
    "\n",
    "plt.subplot(grid[0, 0])\n",
    "sns.lineplot(data=aux1, x=\"month\", y=\"sales\", hue=\"year\")\n",
    "\n",
    "plt.subplot(grid[0, 1])\n",
    "sns.barplot(data=aux2, x=\"month\", y=\"sales\", hue=\"year\")\n",
    "\n",
    "plt.subplot(grid[1, :])\n",
    "sns.lineplot(data=aux3, x=\"date\", y=\"sales\")\n",
    "plt.xticks(ticks=aux3[\"date\"].unique(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H9.** Stores should sell more on the second semester.  \n",
    "**TRUE:** Stores sell **more** in the second semester, specially in december.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"store\", \"year\", \"month\", \"sales\"]]\n",
    "    .groupby([\"store\", \"year\", \"month\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "aux2 = aux1.copy()\n",
    "aux2[\"semester\"] = aux2[\"month\"].apply(lambda x: \"S1\" if x <= 6 else \"S2\")\n",
    "aux2 = aux2.drop([\"month\"], axis=1)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.lineplot(x=\"month\", y=\"sales\", data=aux1)\n",
    "plt.xticks(ticks=aux1[\"month\"].unique())\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x=\"year\", y=\"sales\", data=aux2, hue=\"semester\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H10.** Stores should sell more after 10th day of each month.  \n",
    "**FALSE:** On average, stores **sell a regular amount** throughout the month.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = df4.loc[:, [\"day\", \"sales\"]].groupby([\"day\"]).mean().reset_index()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x=\"day\", y=\"sales\", data=aux1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.regplot(x=\"day\", y=\"sales\", data=aux1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.heatmap(aux1.corr(method=\"pearson\"), annot=True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "aux1[\"before_after\"] = aux1[\"day\"].apply(\n",
    "    lambda x: \"before 10 days\" if x <= 10 else \"after 10 days\"\n",
    ")\n",
    "aux2 = (\n",
    "    aux1.loc[:, [\"before_after\", \"sales\"]]\n",
    "    .groupby([\"before_after\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sns.barplot(x=\"before_after\", y=\"sales\", data=aux2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H11.** Stores should sell less on weekends.  \n",
    "**TRUE:** Stores sell less on weekends mostly because of sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"day_of_week\", \"sales\"]].groupby([\"day_of_week\"]).count().reset_index()\n",
    ")\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.barplot(x=\"day_of_week\", y=\"sales\", data=aux1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.regplot(x=\"day_of_week\", y=\"sales\", data=aux1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.heatmap(aux1.corr(method=\"pearson\"), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **H12.** Stores should sell less during school holidays.  \n",
    "**FALSE:** On average school holidays sell more.  \n",
    "Important to note that on July and August the total amount sold on school holidays are respectively close and higher the regular days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = (\n",
    "    df4.loc[:, [\"school_holiday\", \"sales\"]]\n",
    "    .groupby([\"school_holiday\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x=\"school_holiday\", y=\"sales\", data=aux1)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "aux2 = (\n",
    "    df4.loc[:, [\"month\", \"school_holiday\", \"sales\"]]\n",
    "    .groupby([\"month\", \"school_holiday\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "aux2[\"school_holiday\"] = aux2[\"school_holiday\"].astype(str)\n",
    "\n",
    "sns.barplot(x=\"month\", y=\"sales\", hue=\"school_holiday\", data=aux2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = [\n",
    "    [\"Hypothesis\", \"Conclusion\", \"Relevance\"],\n",
    "    [\"H1\", \"True\", \"Low\"],\n",
    "    [\"H2\", \"False\", \"Medium\"],\n",
    "    [\"H3\", \"False\", \"Medium\"],\n",
    "    [\"H4\", \"False\", \"Low\"],\n",
    "    [\"H5\", \"-\", \"-\"],\n",
    "    [\"H6\", \"False\", \"Low\"],\n",
    "    [\"H7\", \"True\", \"Medium\"],\n",
    "    [\"H8\", \"False\", \"High\"],\n",
    "    [\"H9\", \"True\", \"High\"],\n",
    "    [\"H10\", \"False\", \"High\"],\n",
    "    [\"H11\", \"True\", \"High\"],\n",
    "    [\"H12\", \"False\", \"Low\"],\n",
    "]\n",
    "print(tabulate(tab, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 - Numerical Attrributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = num_attributes.corr(method=\"pearson\")\n",
    "sns.heatmap(correlation, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2- Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cramer V\n",
    "a = df4.select_dtypes(include=\"object\")\n",
    "\n",
    "a1 = cramer_v(a[\"state_holiday\"], a[\"state_holiday\"])\n",
    "a2 = cramer_v(a[\"state_holiday\"], a[\"store_type\"])\n",
    "a3 = cramer_v(a[\"state_holiday\"], a[\"assortment\"])\n",
    "a4 = cramer_v(a[\"store_type\"], a[\"state_holiday\"])\n",
    "a5 = cramer_v(a[\"store_type\"], a[\"store_type\"])\n",
    "a6 = cramer_v(a[\"store_type\"], a[\"assortment\"])\n",
    "a7 = cramer_v(a[\"assortment\"], a[\"state_holiday\"])\n",
    "a8 = cramer_v(a[\"assortment\"], a[\"store_type\"])\n",
    "a9 = cramer_v(a[\"assortment\"], a[\"assortment\"])\n",
    "\n",
    "# Final dataset\n",
    "d = pd.DataFrame(\n",
    "    {\n",
    "        \"state_holiday\": [a1, a2, a3],\n",
    "        \"store_type\": [a4, a5, a6],\n",
    "        \"assortment\": [a7, a8, a9],\n",
    "    }\n",
    ")\n",
    "d = d.set_index(d.columns)\n",
    "sns.heatmap(d, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome of the variables have Gaussian Distribution, so normalization is not needed.  \n",
    "This can be seen on seen on univariable analysis for numerical attributes on section 4.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df5.select_dtypes(include=[\"int64\", \"float64\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x=a[\"competition_distance\"])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=a[\"competition_time_month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale time features\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# competition distance\n",
    "df5[\"competition_distance\"] = rs.fit_transform(df5[[\"competition_distance\"]].values)\n",
    "pickle.dump(rs, open(\"../parameters/competition_distance_scaler.pkl\", \"wb\"))\n",
    "\n",
    "# competition time month\n",
    "df5[\"competition_time_month\"] = rs.fit_transform(df5[[\"competition_time_month\"]].values)\n",
    "pickle.dump(rs, open(\"../parameters/competition_time_month_scaler.pkl\", \"wb\"))\n",
    "\n",
    "# promo time week\n",
    "df5[\"promo_time_week\"] = mms.fit_transform(df5[[\"promo_time_week\"]].values)\n",
    "pickle.dump(mms, open(\"../parameters/promo_time_week_scaler.pkl\", \"wb\"))\n",
    "\n",
    "# year\n",
    "df5[\"year\"] = mms.fit_transform(df5[[\"year\"]].values)\n",
    "pickle.dump(mms, open(\"../parameters/year_scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies(df5, prefix=[\"state_holiday\"], columns=[\"state_holiday\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store type - Label Encoder\n",
    "le = LabelEncoder()\n",
    "df5[\"store_type\"] = le.fit_transform(df5[\"store_type\"])\n",
    "pickle.dump(le, open(\"../parameters/store_type_scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assortment - Ordinal Encoding\n",
    "assortment_dict = {\n",
    "    \"basic\": 1,\n",
    "    \"extra\": 2,\n",
    "    \"extended\": 3,\n",
    "}\n",
    "\n",
    "df5[\"assortment\"] = df5[\"assortment\"].map(assortment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 - Response Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[\"sales\"] = np.log1p(df5[\"sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df4[\"sales\"], bins=25, kde=True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df5[\"sales\"], bins=25, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 - Natural Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month\n",
    "df5[\"month_sin\"] = df5[\"month\"].apply(lambda x: np.sin(x * (2.0 * np.pi / 12)))\n",
    "df5[\"month_cos\"] = df5[\"month\"].apply(lambda x: np.cos(x * (2.0 * np.pi / 12)))\n",
    "\n",
    "# day\n",
    "df5[\"day_sin\"] = df5[\"day\"].apply(lambda x: np.sin(x * (2.0 * np.pi / 30)))\n",
    "df5[\"day_cos\"] = df5[\"day\"].apply(lambda x: np.cos(x * (2.0 * np.pi / 30)))\n",
    "\n",
    "# day of week\n",
    "df5[\"day_of_week_sin\"] = df5[\"day_of_week\"].apply(\n",
    "    lambda x: np.sin(x * (2.0 * np.pi / 7))\n",
    ")\n",
    "df5[\"day_of_week_cos\"] = df5[\"day_of_week\"].apply(\n",
    "    lambda x: np.cos(x * (2.0 * np.pi / 7))\n",
    ")\n",
    "\n",
    "# week of year\n",
    "df5[\"week_of_year_sin\"] = df5[\"week_of_year\"].apply(\n",
    "    lambda x: np.sin(x * (2.0 * np.pi / 52))\n",
    ")\n",
    "df5[\"week_of_year_cos\"] = df5[\"week_of_year\"].apply(\n",
    "    lambda x: np.cos(x * (2.0 * np.pi / 52))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_drop = [\n",
    "    \"day_of_week\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"week_of_year\",\n",
    "    \"promo_since\",\n",
    "    \"competition_since\",\n",
    "    \"year_week\",\n",
    "]\n",
    "\n",
    "df6 = df6.drop(cols_drop, axis=1)\n",
    "\n",
    "df6.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 - Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df6.copy()\n",
    "y_train = X_train[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Test Dataset\n",
    "X_test = test.copy()\n",
    "\n",
    "X_test = rename_cols(X_test).copy()\n",
    "X_test = fillout_na(X_test).copy()\n",
    "X_test = change_types(X_test).copy()\n",
    "X_test = feature_eng(X_test).copy()\n",
    "X_test = filter_features(X_test).copy()\n",
    "X_test = data_preparation(X_test).copy()\n",
    "\n",
    "y_test = X_test[\"sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - Boruta Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and Test datasets for Boruta\n",
    "# X_train_n = X_train.drop([\"date\", \"sales\"], axis=1).values\n",
    "# y_train_n = y_train.values.ravel()\n",
    "\n",
    "# # Define RandonForestRegressor\n",
    "# rf = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# # Define Boruta\n",
    "# boruta = BorutaPy(rf, n_estimators=\"auto\", verbose=2, random_state=42).fit(\n",
    "#     X_train_n, y_train_n\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 - Best Features From Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# # Best Features\n",
    "# X_train_fs = X_train.drop([\"date\", \"sales\"], axis=1)\n",
    "# cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "# # Features Not Selected\n",
    "# cols_not_selected_boruta = list(np.setdiff1d(X_train_fs.columns, cols_selected_boruta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_selected_boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta = [\n",
    "    \"store\",\n",
    "    \"promo\",\n",
    "    \"store_type\",\n",
    "    \"assortment\",\n",
    "    \"competition_distance\",\n",
    "    \"competition_open_since_month\",\n",
    "    \"competition_open_since_year\",\n",
    "    \"promo2\",\n",
    "    \"promo2_since_week\",\n",
    "    \"promo2_since_year\",\n",
    "    \"competition_time_month\",\n",
    "    \"promo_time_week\",\n",
    "    \"month_sin\",\n",
    "    \"month_cos\",\n",
    "    \"day_sin\",\n",
    "    \"day_cos\",\n",
    "    \"day_of_week_sin\",\n",
    "    \"day_of_week_cos\",\n",
    "    \"week_of_year_sin\",\n",
    "    \"week_of_year_cos\",\n",
    "]\n",
    "\n",
    "# added \"month_sin\" and \"week_of_year_sin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_not_selected_boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_not_selected_boruta = [\n",
    "    \"is_promo\",\n",
    "    \"school_holiday\",\n",
    "    \"state_holiday_christmas\",\n",
    "    \"state_holiday_easter_holiday\",\n",
    "    \"state_holiday_public_holiday\",\n",
    "    \"state_holiday_regular_day\",\n",
    "    \"year\",\n",
    "]\n",
    "\n",
    "# consider use \"year\" feature in future model tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend([\"date\", \"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 - Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[cols_selected_boruta]\n",
    "x_test = X_test[cols_selected_boruta]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_train_full = X_train[cols_selected_boruta_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 - Average Model - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1[\"sales\"] = y_test.copy()\n",
    "\n",
    "# Prediction\n",
    "aux2 = (\n",
    "    aux1.loc[:, [\"store\", \"sales\"]]\n",
    "    .groupby(\"store\")\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sales\": \"predictions\"})\n",
    ")\n",
    "aux1 = pd.merge(aux1, aux2, how=\"left\", on=\"store\")\n",
    "yhat_baseline = aux1[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "baseline_performace = ml_error(\n",
    "    \"Average Model\", np.expm1(y_test), np.expm1(yhat_baseline)\n",
    ")\n",
    "baseline_performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "yhat_lr = lr.predict(x_test)\n",
    "\n",
    "# Performance\n",
    "lr_performance = ml_error(\"Linear Regression\", np.expm1(y_test), np.expm1(yhat_lr))\n",
    "lr_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 - Linear Regression - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_performance_cv = cross_validation(x_train_full, 5, \"Linear Regression\", lr)\n",
    "lr_performance_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 - Linear Regression - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "lrr = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "# Performance\n",
    "lrr_performance = ml_error(\"Lasso\", np.expm1(y_test), np.expm1(yhat_lrr))\n",
    "lrr_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 - Lasso - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr_performance_cv = cross_validation(x_train_full, 5, \"Lasso\", lrr)\n",
    "lrr_performance_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 - Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(x_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "yhat_rf = rf.predict(x_test)\n",
    "\n",
    "# Performance\n",
    "rf_performance = ml_error(\"Random Forest\", np.expm1(y_test), np.expm1(yhat_rf))\n",
    "rf_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 - Random Forest - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_performance_cv = cross_validation(\n",
    "    x_train_full, 5, \"Random Forest\", rf, verbose=False\n",
    ")\n",
    "rf_performance_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 - XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "xgb_model = xgb.XGBRegressor().fit(x_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "yhat_xgb = xgb_model.predict(x_test)\n",
    "\n",
    "# Performance\n",
    "xgb_performance = ml_error(\"XGBoost\", np.expm1(y_test), np.expm1(yhat_xgb))\n",
    "xgb_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 - XGBoost - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_performance_cv = cross_validation(\n",
    "    x_train_full, 5, \"XGBoost\", xgb_model, verbose=False\n",
    ")\n",
    "xgb_performance_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 - Compare Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1 - Basic Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_performance = pd.concat(\n",
    "    [\n",
    "        baseline_performace,\n",
    "        lr_performance,\n",
    "        lrr_performance,\n",
    "        rf_performance,\n",
    "        xgb_performance,\n",
    "    ]\n",
    ")\n",
    "models_performance.sort_values(\"RMSE\", inplace=True)\n",
    "models_performance.reset_index(inplace=True, drop=True)\n",
    "models_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Linear Models do not perform better than a average model, which means this data needs a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "models_performance.to_csv(\"../data/processed/performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2 - Cross Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_performance_cv = pd.concat(\n",
    "    [\n",
    "        lr_performance_cv,\n",
    "        lrr_performance_cv,\n",
    "        rf_performance_cv,\n",
    "        xgb_performance_cv,\n",
    "    ]\n",
    ")\n",
    "models_performance_cv.sort_values(\"RMSE CV\", inplace=True)\n",
    "models_performance_cv.reset_index(inplace=True, drop=True)\n",
    "models_performance_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "models_performance_cv.to_csv(\"../data/processed/performance_cv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 - Hyperparameter Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": [1500, 1700, 2500, 3000, 3500],\n",
    "    \"eta\": [0.01, 0.03, 0.05, 0.07, 0.09],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"subsample\": [0.1, 0.3, 0.5, 0.7],\n",
    "    \"colsample_bytree\": [0.3, 0.7, 0.9],\n",
    "    \"min_child_weight\": [3, 8, 15],\n",
    "}\n",
    "\n",
    "MAX_EVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_performance = pd.DataFrame()\n",
    "\n",
    "for i in range(MAX_EVAL):\n",
    "    # choose parameters randomly\n",
    "    hp = {k: random.sample(v, 1)[0] for k, v in params.items()}\n",
    "    print(hp)\n",
    "\n",
    "    # Model\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=hp[\"n_estimators\"],\n",
    "        eta=hp[\"eta\"],\n",
    "        max_depth=hp[\"max_depth\"],\n",
    "        subsample=hp[\"subsample\"],\n",
    "        colsample_bytree=hp[\"colsample_bytree\"],\n",
    "        min_child_weight=hp[\"min_child_weight\"],\n",
    "    )\n",
    "\n",
    "    # Performance\n",
    "    xgb_performance = cross_validation(\n",
    "        x_train_full, 5, \"XGBoost Regressor\", xgb_model, verbose=False\n",
    "    )\n",
    "    final_performance = pd.concat([final_performance, xgb_performance])\n",
    "\n",
    "final_performance.reset_index(inplace=True, drop=True)\n",
    "final_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "final_performance.to_csv(\"../data/processed/final_performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 - Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tuned = {\n",
    "    \"n_estimators\": 1700,\n",
    "    \"eta\": 0.05,\n",
    "    \"max_depth\": 9,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"min_child_weight\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "xgb_model_tuned = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=params_tuned[\"n_estimators\"],\n",
    "    eta=params_tuned[\"eta\"],\n",
    "    max_depth=params_tuned[\"max_depth\"],\n",
    "    subsample=params_tuned[\"subsample\"],\n",
    "    colsample_bytree=params_tuned[\"colsample_bytree\"],\n",
    "    min_child_weight=params_tuned[\"min_child_weight\"],\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "yhat_xgb_tuned = xgb_model_tuned.predict(x_test)\n",
    "\n",
    "# Performance\n",
    "xgb_performance_tuned = ml_error(\n",
    "    \"XGBoost Regressor\", np.expm1(y_test), np.expm1(yhat_xgb_tuned)\n",
    ")\n",
    "xgb_performance_tuned.reset_index(inplace=True, drop=True)\n",
    "xgb_performance_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "pickle.dump(xgb_model_tuned, open(\"../models/xgb_rossmann.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv\n",
    "xgb_performance_tuned.to_csv(\"../data/processed/xgb_performance_tuned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 - Error Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = X_test[cols_selected_boruta_full]\n",
    "\n",
    "# rescale\n",
    "df9[\"sales\"] = np.expm1(df9[\"sales\"])\n",
    "df9[\"predictions\"] = np.expm1(yhat_xgb_tuned)\n",
    "\n",
    "df91 = df9.loc[:, [\"store\", \"predictions\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 - Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR = xgb_performance_tuned[\"MAPE\"][0]\n",
    "BEST_ERROR = 1 + ERROR\n",
    "WORST_ERROR = 1 - ERROR\n",
    "\n",
    "# sum of predictions\n",
    "df92 = df91.groupby(\"store\").sum().reset_index()\n",
    "\n",
    "df92[\"best_scenario\"] = df92[\"predictions\"] * BEST_ERROR\n",
    "df92[\"worst_scenario\"] = df92[\"predictions\"] * WORST_ERROR\n",
    "df92[\"MAE\"] = df92[\"best_scenario\"] - df92[\"predictions\"]\n",
    "df92.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE for each store\n",
    "df9_aux = (\n",
    "    df9.loc[:, [\"store\", \"sales\", \"predictions\"]]\n",
    "    .groupby(\"store\")\n",
    "    .apply(lambda x: mean_absolute_percentage_error(x[\"sales\"], x[\"predictions\"]))\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"MAPE\"})\n",
    ")\n",
    "\n",
    "# Merge\n",
    "df93 = pd.merge(df92, df9_aux, how=\"inner\", on=\"store\")\n",
    "\n",
    "sns.scatterplot(data=df93, x=\"store\", y=\"MAPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 - Total Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df94 = (\n",
    "    df92.loc[:, [\"predictions\", \"worst_scenario\", \"best_scenario\"]]\n",
    "    .apply(lambda x: np.sum(x), axis=0)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"Scenario\", 0: \"Values\"})\n",
    ")\n",
    "df94[\"Values\"] = df94[\"Values\"].map(\" {:,.2f}\".format)\n",
    "df94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 - Machine Learning Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9[\"error\"] = df9[\"sales\"] - df9[\"predictions\"]\n",
    "df9[\"error_rate\"] = df9[\"predictions\"] / df9[\"sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "sns.lineplot(data=df9, x=\"date\", y=\"sales\", label=\"SALES\")\n",
    "sns.lineplot(data=df9, x=\"date\", y=\"predictions\", label=\"PREDICTIONS\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.lineplot(data=df9, x=\"date\", y=\"error_rate\")\n",
    "plt.axhline(1, linestyle=\"--\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.distplot(df9[\"error\"])\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.scatterplot(x=df9[\"predictions\"], y=df9[\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Deploy Model to Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 - API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# loading test dataset\n",
    "df10 = pd.read_csv(\"../data/raw/test.csv\")\n",
    "df_store_raw = pd.read_csv(\"../data/raw/store.csv\")\n",
    "\n",
    "# Merge testa and store dataset\n",
    "df_test = pd.merge(df10, df_store_raw, how=\"left\", on=\"Store\")\n",
    "\n",
    "# Choose store for prediction\n",
    "df_test = df_test[df_test[\"Store\"] == 42]\n",
    "\n",
    "# Remove closed days\n",
    "df_test = df_test[df_test[\"Open\"] != 0]\n",
    "df_test = df_test[~df_test[\"Open\"].isnull()]\n",
    "\n",
    "# Remove Id\n",
    "df_test = df_test.drop(\"Id\", axis=1)\n",
    "\n",
    "# Convert dataframe to json\n",
    "data = df_test.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "# API call\n",
    "# url = \"http://0.0.0.0:8000/rossmann/predict\"\n",
    "url = \"https://rossmann-6xtx.onrender.com/rossmann/predict\"\n",
    "\n",
    "# Make a POST request to the API\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Check the response\n",
    "print(f\"Status Code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store Number 42 will sell  272,767.39 in the next 6 weeks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26953/2909690222.py:2: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  d1 = pd.read_json(response.json())\n"
     ]
    }
   ],
   "source": [
    "# Transform results to dataframe\n",
    "d1 = pd.read_json(response.json())\n",
    "\n",
    "d2 = d1.loc[:, [\"store\", \"prediction\"]].groupby(\"store\").sum().reset_index()\n",
    "\n",
    "for i in range(len(d2)):\n",
    "    store = d2.loc[i, \"store\"]\n",
    "    prediction = d2.loc[i, \"prediction\"]\n",
    "    print(f\"Store Number {store} will sell  {prediction:,.2f} in the next 6 weeks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_producao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
